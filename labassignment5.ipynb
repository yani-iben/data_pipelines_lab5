{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 5: Web Scraping\n",
    "## DS 6001\n",
    "\n",
    "### Instructions\n",
    "Please answer the following questions as completely as possible using text, code, and the results of code as needed. Format your answers in a Jupyter notebook. To receive full credit, make sure you address every part of the problem, and make sure your document is formatted in a clean and professional way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 1\n",
    "Large language models are very impressive. They use deep neural networks with billions of parameters. They fine-tune results with clever approaches to reinforcement learning through human feedback. Many of them employ APIs with user interfaces that allow users to chat in natural language through a simple textbox, and receive a response in only a few seconds. And the societal impact of these models is undeniably enormous, to an extent we are only beginning to understand.\n",
    "\n",
    "But, none of that is the most impressive part of LLMs. Like anything else in machine learning, the most impressive and difficult element of the work is the data collection.\n",
    "\n",
    "The data used to train major LLMs is something that big companies communicate very little about. [OpenAI](https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-foundation-models-are-developed) only says that GPT and other baseline models are trained on data that is \"freely and openly accessible on the internet.\" Many commentators, such as Dr. Vered Shwartz, assistant professor in the University of British Columbia department of computer science, say that LLMs like GPT are trained on \"[essentially the entire internet](https://science.ubc.ca/news/chatgpt-has-read-almost-whole-internet-hasnt-solved-its-diversity-issues).\" Think about pulling the entire internet into a single model and tell me this isn't the most impressive thing about ChatGPT.\n",
    "\n",
    "A primary training set for Open AI's GPT and other major LLMs is the corpus compiled by [Common Crawl](https://commoncrawl.org/faq), a nonprofit organization that describes itself as \"dedicated to providing a copy of the Internet to Internet researchers, companies and individuals at no cost for the purpose of research and analysis.\" According to the [Mozilla Foundation](https://www.mozillafoundation.org/en/blog/Mozilla-Report-How-Common-Crawl-Data-Infrastructure-Shaped-the-Battle-Royale-over-Generative-AI/), \"[o]ver 80% of GPT-3 tokens (a representation unit of text data) stemmed from Common Crawl. Many models published by other developers likewise rely heavily on it: the study analyzed 47 LLMs published between 2019 and October 2023 that power text generators and found at least 64% of them were trained on Common Crawl.\"\n",
    "\n",
    "Common Crawl is a massive web scraping endeavor. But it's not necessarily a sophisticated one. Mostly, Common Crawl is downloading the raw HTML from the webpages it scrapes and extracting text from the HTML. A task like this is exactly the kind of thing we can use `requests` and `BeautifulSoup` to do. \n",
    "\n",
    "Common Crawl is also at the center of many controversies and legal actions regarding generative AI, such as the New York Times' copyright infringement [lawsuit](https://www.npr.org/2025/03/26/nx-s1-5288157/new-york-times-openai-copyright-case-goes-forward) against OpenAI, and concerns about bias and racism in LLMs stemming from their training data.\n",
    "\n",
    "For this problem, please examine the [Common Crawl website](https://commoncrawl.org/) and read \"[Training Data for the Price of a Sandwich: Common Crawl’s Impact on Generative AI](https://assets.mofoprod.net/network/documents/Common_Crawl_Mozilla_Foundation_2024.pdf)\" by Stefan Baack for the Mozilla Foundation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "If you want access to the latest Common Crawl dataset, where can you get it? How many website does the dataset contain? And outside of computational and storage costs, how much will it cost to obtain the data? (Use [Common Crawl's website](https://commoncrawl.org/) to answer this question) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the \"get started\" page to understand how to access the latest Common Crawl dataset. The data is available on the AWS Open Source Sets Sponsorships program under the s3 bucket. The latest dataset, from September 2025, contains 2.39 billion website pages. \n",
    "\n",
    "In terms of cost, the crawl data is free to access and obtain. The computational and storage costs are associated with the network or storage fees for downloading large amounts of data locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Is Common Crawl more useful for the pre-training or fine-tuning stage of the development of an LLM? Why is the Common Crawl corpus used by so many AI efforts, and why must it usually be altered or filtered in some way? (see pages 11-13 of Baack's article) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Crawl is more useful for the fine-tuning stage of the development sinze Common Crawl can be used to scale up pre-training data with filtered data. Common Crawl is used by so many AI efforts becaus enot including it would be too much of a departure and risk to undermining comparisons with LLMs that were previously released. Also, the data is considered more diverse and vast.\n",
    "\n",
    "The data must often be altered in some way because the data can contain content that is problematic or undesirable for AI training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Given how Common Crawl decides which URLs to crawl, what are three reasons why the data cannot be said to be the complete internet or a representative sample of it? (see pages 17-22 of Baack's article) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Crawl's method of deciding which URLs ro crow is based on calculating harmonic centrality. This means measuring a domain's importance based on how often the domain is connected by other nodes within a network. This means that Common Crawl's method will favor domains that are more popular. This can cause the data to miss out on variety.\n",
    "\n",
    "Second, Common Crawl follows the robots.txt protocol, which lets website admin to block crawl users. This means that websites like Facebook and the New York Times can block Common Crawl from having access to most of their information. As a result, Common Crawl's data can have serious gaps in information from large platforms. \n",
    "\n",
    "Third, Common Crawl's infrastructure is biased towards to the U.S. Because Common Crawl is geographically focused on the U.S the content can also be biased towards content that uses English. This prevents content from accurately representing content in other languges are content that follows cultures outside of the U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Of the suggestions that the Mozilla Foundation make for the future development of Common Crawl, are there any that you especially agree with or disagree with, and why? (See pages 30-31 of Baack's article) [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 2\n",
    "For the following problems, you will be scraping http://books.toscrape.com/. This website is a fake book retailer, designed to mimic the design of many retail websites. It exists solely to help students practice web-scraping, so there aren’t going to be any ethical concerns with this particular exercise, and there shouldn’t be any issues with rate limits or other gates that could prevent web-scraping. Take a moment and look at this website, so that you know what you will be working with.\n",
    "\n",
    "Your goal is to generate a dataframe with four columns: one for the title, one for the price, one for the star-rating, and one or the book cover JPEG’s URL. The dataframe will also 1000 rows, one for each of the 1000 books listed on the 50 pages of this website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "Pull the HTML code from http://books.toscrape.com/. Make sure you provide the correct user agent string. Then parse this HTML code and save the parsed code as a separate Python variable. [8 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {'user-agent': 'ds600 (dkn4gw@virginia.edu)'}\n",
    "r = requests.get(\"https://books.toscrape.com/\",headers=headers)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Extract all 20 of the book titles and save them in a list. [8 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = BeautifulSoup(r.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlelist=books.find_all(\"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Light in the ...', 'Tipping the Velvet', 'Soumission', 'Sharp Objects', 'Sapiens: A Brief History ...', 'The Requiem Red', 'The Dirty Little Secrets ...', 'The Coming Woman: A ...', 'The Boys in the ...', 'The Black Maria', 'Starving Hearts (Triangular Trade ...', \"Shakespeare's Sonnets\", 'Set Me Free', \"Scott Pilgrim's Precious Little ...\", 'Rip it Up and ...', 'Our Band Could Be ...', 'Olio', 'Mesaerion: The Best Science ...', 'Libertarianism for Beginners', \"It's Only the Himalayas\"]\n"
     ]
    }
   ],
   "source": [
    "btags_title = books.find_all(\"h3\")\n",
    "blist = [b.string for b in btags_title]\n",
    "print(blist[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Extract the price of each of the 20 books and save these prices in a list. (The prices are listed in British pounds, and include the £ symbol. Remove the £ symbols: if you’ve saved the prices in a list named `prices`, then the following code should work: `prices = [s.replace('Â£', '') for s in prices]`.) [8 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['51.77', '53.74', '50.10', '47.82', '54.23', '22.65', '33.34', '17.93', '22.60', '52.15', '13.99', '20.66', '17.46', '52.29', '35.02', '57.25', '23.88', '37.59', '51.33', '45.17']\n"
     ]
    }
   ],
   "source": [
    "ptags_price = books.find_all(\"p\", class_=\"price_color\")\n",
    "plist = [p.get_text(strip=True).replace('Â£', '') for p in ptags_price]\n",
    "print(plist[0:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Extract the star level ratings for the 20 books. [Hint: for tags such as `<p class=\"star-rating One\">` in which the class has a space, the class is actually a list in which the first item in the list is `\"star-rating\"` and the second item in the list is `\"One\"`. It's possible to search on either item in this list.][8 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Three',\n",
       " 'One',\n",
       " 'One',\n",
       " 'Four',\n",
       " 'Five',\n",
       " 'One',\n",
       " 'Four',\n",
       " 'Three',\n",
       " 'Four',\n",
       " 'One',\n",
       " 'Two',\n",
       " 'Four',\n",
       " 'Five',\n",
       " 'Five',\n",
       " 'Five',\n",
       " 'Three',\n",
       " 'One',\n",
       " 'One',\n",
       " 'Two',\n",
       " 'Two']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_tags = books.find_all(\"p\", class_=\"star-rating\")\n",
    "slist = [s[\"class\"][1] for s in star_tags]\n",
    "star_tags\n",
    "slist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e\n",
    "Extract the URLs for the JPEG thumbnail images that show the covers of the 20 books. (Maybe we want to mine the images to build models that predict the star level, literally judging books by their covers.) [8 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg',\n",
       " 'media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f4a1c.jpg',\n",
       " 'media/cache/3e/ef/3eef99c9d9adef34639f510662022830.jpg',\n",
       " 'media/cache/32/51/3251cf3a3412f53f339e42cac2134093.jpg',\n",
       " 'media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c12a6.jpg',\n",
       " 'media/cache/68/33/68339b4c9bc034267e1da611ab3b34f8.jpg',\n",
       " 'media/cache/92/27/92274a95b7c251fea59a2b8a78275ab4.jpg',\n",
       " 'media/cache/3d/54/3d54940e57e662c4dd1f3ff00c78cc64.jpg',\n",
       " 'media/cache/66/88/66883b91f6804b2323c8369331cb7dd1.jpg',\n",
       " 'media/cache/58/46/5846057e28022268153beff6d352b06c.jpg',\n",
       " 'media/cache/be/f4/bef44da28c98f905a3ebec0b87be8530.jpg',\n",
       " 'media/cache/10/48/1048f63d3b5061cd2f424d20b3f9b666.jpg',\n",
       " 'media/cache/5b/88/5b88c52633f53cacf162c15f4f823153.jpg',\n",
       " 'media/cache/94/b1/94b1b8b244bce9677c2f29ccc890d4d2.jpg',\n",
       " 'media/cache/81/c4/81c4a973364e17d01f217e1188253d5e.jpg',\n",
       " 'media/cache/54/60/54607fe8945897cdcced0044103b10b6.jpg',\n",
       " 'media/cache/55/33/553310a7162dfbc2c6d19a84da0df9e1.jpg',\n",
       " 'media/cache/09/a3/09a3aef48557576e1a85ba7efea8ecb7.jpg',\n",
       " 'media/cache/0b/bc/0bbcd0a6f4bcd81ccb1049a52736406e.jpg',\n",
       " 'media/cache/27/a5/27a53d0bb95bdd88288eaf66c9230d7e.jpg']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jpg_tags = books.find_all(\"div\", \"a\",class_=\"image_container\")\n",
    "jlist = [j.find(\"img\")[\"src\"] for j in jpg_tags]\n",
    "jlist[0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part f\n",
    "Create a dataframe with one row for each of the 20 books, and the book titles, prices, star ratings, and cover JPEG URLs as the four columns. [8 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict={\n",
    "    \"title\":blist,\n",
    "    \"price\":plist,\n",
    "    \"star_rating\":slist,\n",
    "    \"image_url\":jlist\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>Â£51.77</td>\n",
       "      <td>Three</td>\n",
       "      <td>media/cache/2c/da/2cdad67c44b002e7ead0cc35693c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>Â£53.74</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>Â£50.10</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/3e/ef/3eef99c9d9adef34639f51066202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>Â£47.82</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/32/51/3251cf3a3412f53f339e42cac213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>Â£54.23</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>Â£22.65</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/68/33/68339b4c9bc034267e1da611ab3b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets ...</td>\n",
       "      <td>Â£33.34</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/92/27/92274a95b7c251fea59a2b8a7827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A ...</td>\n",
       "      <td>Â£17.93</td>\n",
       "      <td>Three</td>\n",
       "      <td>media/cache/3d/54/3d54940e57e662c4dd1f3ff00c78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the ...</td>\n",
       "      <td>Â£22.60</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/66/88/66883b91f6804b2323c8369331cb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>Â£52.15</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/58/46/5846057e28022268153beff6d352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade ...</td>\n",
       "      <td>Â£13.99</td>\n",
       "      <td>Two</td>\n",
       "      <td>media/cache/be/f4/bef44da28c98f905a3ebec0b87be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>Â£20.66</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/10/48/1048f63d3b5061cd2f424d20b3f9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>Â£17.46</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/5b/88/5b88c52633f53cacf162c15f4f82...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little ...</td>\n",
       "      <td>Â£52.29</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/94/b1/94b1b8b244bce9677c2f29ccc890...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and ...</td>\n",
       "      <td>Â£35.02</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/81/c4/81c4a973364e17d01f217e118825...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be ...</td>\n",
       "      <td>Â£57.25</td>\n",
       "      <td>Three</td>\n",
       "      <td>media/cache/54/60/54607fe8945897cdcced0044103b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>Â£23.88</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/55/33/553310a7162dfbc2c6d19a84da0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science ...</td>\n",
       "      <td>Â£37.59</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/09/a3/09a3aef48557576e1a85ba7efea8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>Â£51.33</td>\n",
       "      <td>Two</td>\n",
       "      <td>media/cache/0b/bc/0bbcd0a6f4bcd81ccb1049a52736...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>Â£45.17</td>\n",
       "      <td>Two</td>\n",
       "      <td>media/cache/27/a5/27a53d0bb95bdd88288eaf66c923...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title    price star_rating  \\\n",
       "0                      A Light in the ...  Â£51.77       Three   \n",
       "1                      Tipping the Velvet  Â£53.74         One   \n",
       "2                              Soumission  Â£50.10         One   \n",
       "3                           Sharp Objects  Â£47.82        Four   \n",
       "4            Sapiens: A Brief History ...  Â£54.23        Five   \n",
       "5                         The Requiem Red  Â£22.65         One   \n",
       "6            The Dirty Little Secrets ...  Â£33.34        Four   \n",
       "7                 The Coming Woman: A ...  Â£17.93       Three   \n",
       "8                     The Boys in the ...  Â£22.60        Four   \n",
       "9                         The Black Maria  Â£52.15         One   \n",
       "10  Starving Hearts (Triangular Trade ...  Â£13.99         Two   \n",
       "11                  Shakespeare's Sonnets  Â£20.66        Four   \n",
       "12                            Set Me Free  Â£17.46        Five   \n",
       "13    Scott Pilgrim's Precious Little ...  Â£52.29        Five   \n",
       "14                      Rip it Up and ...  Â£35.02        Five   \n",
       "15                  Our Band Could Be ...  Â£57.25       Three   \n",
       "16                                   Olio  Â£23.88         One   \n",
       "17        Mesaerion: The Best Science ...  Â£37.59         One   \n",
       "18           Libertarianism for Beginners  Â£51.33         Two   \n",
       "19                It's Only the Himalayas  Â£45.17         Two   \n",
       "\n",
       "                                            image_url  \n",
       "0   media/cache/2c/da/2cdad67c44b002e7ead0cc35693c...  \n",
       "1   media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f...  \n",
       "2   media/cache/3e/ef/3eef99c9d9adef34639f51066202...  \n",
       "3   media/cache/32/51/3251cf3a3412f53f339e42cac213...  \n",
       "4   media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c...  \n",
       "5   media/cache/68/33/68339b4c9bc034267e1da611ab3b...  \n",
       "6   media/cache/92/27/92274a95b7c251fea59a2b8a7827...  \n",
       "7   media/cache/3d/54/3d54940e57e662c4dd1f3ff00c78...  \n",
       "8   media/cache/66/88/66883b91f6804b2323c8369331cb...  \n",
       "9   media/cache/58/46/5846057e28022268153beff6d352...  \n",
       "10  media/cache/be/f4/bef44da28c98f905a3ebec0b87be...  \n",
       "11  media/cache/10/48/1048f63d3b5061cd2f424d20b3f9...  \n",
       "12  media/cache/5b/88/5b88c52633f53cacf162c15f4f82...  \n",
       "13  media/cache/94/b1/94b1b8b244bce9677c2f29ccc890...  \n",
       "14  media/cache/81/c4/81c4a973364e17d01f217e118825...  \n",
       "15  media/cache/54/60/54607fe8945897cdcced0044103b...  \n",
       "16  media/cache/55/33/553310a7162dfbc2c6d19a84da0d...  \n",
       "17  media/cache/09/a3/09a3aef48557576e1a85ba7efea8...  \n",
       "18  media/cache/0b/bc/0bbcd0a6f4bcd81ccb1049a52736...  \n",
       "19  media/cache/27/a5/27a53d0bb95bdd88288eaf66c923...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df=pd.DataFrame(book_dict)\n",
    "book_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part g\n",
    "Create a function that takes the URL of the webpage to scrape as an input, applies the code you wrote for parts a through e, and generates the dataframe from part f as the output. [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape(url):\n",
    "    headers={'user-agent': 'ds600 (dkn4gw@virginia.edu)'}\n",
    "    r = requests.get(url,headers=headers)\n",
    "    books = BeautifulSoup(r.text, 'html')\n",
    "    btags_title = books.find_all(\"h3\")\n",
    "    blist = [b.string for b in btags_title]\n",
    "    ptags_price = books.find_all(\"p\", class_=\"price_color\")\n",
    "    plist = [p.get_text(strip=True).replace('Â£', '') for p in ptags_price]\n",
    "    star_tags = books.find_all(\"p\", class_=\"star-rating\")\n",
    "    slist = [s[\"class\"][1] for s in star_tags]\n",
    "    jpg_tags = books.find_all(\"div\", \"a\",class_=\"image_container\")\n",
    "    jlist = [j.find(\"img\")[\"src\"] for j in jpg_tags]\n",
    "    book_dict={\n",
    "        \"title\":blist[0:20],\n",
    "        \"price\":plist[0:20],\n",
    "        \"star_rating\":slist[0:20],\n",
    "        \"image_url\":jlist[0:20]\n",
    "    }\n",
    "    book_df=pd.DataFrame(book_dict)\n",
    "    return book_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part h\n",
    "Notice that there are many pages to http://books.toscrape.com/. When you click on “Next” in the bottom-right corner of the screen, it takes you to http://books.toscrape.com/catalogue/page-2.html. The front page is the same as http://books.toscrape.com/catalogue/page-1.html, and there are 50 total pages.\n",
    "\n",
    "Write a loop that uses the function you wrote in part g to scrape each of the 50 pages, and append each of these data frames together. If you write this loop correctly, your dataframe will have 1000 rows (20 books on each of the 50 pages). \n",
    "\n",
    "Some hints:\n",
    "\n",
    "* Typing `new_df = pd.DataFrame()` with nothing in the parentheses will create an empty data frame on which new data can be appended.\n",
    "\n",
    "* There are many loops you can use, but the most straightforward one is a for-values loop that counts from 1 to 50. In Python, you can initialize such a loop with for i in range(1, 51):, and indenting every line below it that belongs inside the loop. Inside the loop, the letter i is now a stand-in for the number currently being considered.\n",
    "\n",
    "* You will need to figure out how to replace the number in URLs like http://books.toscrape.com/catalogue/page-2.html with the number currently under consideration in the loop. You might need the `str()` function, which turns numeric values into strings.\n",
    "\n",
    "* `pd.concat()` is a method that appends dataframes together.\n",
    "\n",
    "[10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,50):\n",
    "    if i==0:\n",
    "        url=\"https://books.toscrape.com/\"\n",
    "    else:\n",
    "        url=f\"https://books.toscrape.com/catalogue/page-{i+1}.html\"\n",
    "    df=web_scrape(url)\n",
    "    if i==0:\n",
    "        final_df=df\n",
    "    else:\n",
    "        final_df=pd.concat([final_df,df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>51.77</td>\n",
       "      <td>Three</td>\n",
       "      <td>media/cache/2c/da/2cdad67c44b002e7ead0cc35693c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/3e/ef/3eef99c9d9adef34639f51066202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/32/51/3251cf3a3412f53f339e42cac213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>54.23</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Alice in Wonderland (Alice's ...</td>\n",
       "      <td>55.53</td>\n",
       "      <td>One</td>\n",
       "      <td>../media/cache/96/ee/96ee77d71a31b7694dac6855f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Ajin: Demi-Human, Volume 1 ...</td>\n",
       "      <td>57.06</td>\n",
       "      <td>Four</td>\n",
       "      <td>../media/cache/09/7c/097cb5ecc6fb3fbe1690cf0cb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>A Spy's Devotion (The ...</td>\n",
       "      <td>16.97</td>\n",
       "      <td>Five</td>\n",
       "      <td>../media/cache/1b/5f/1b5ff86f3c75e51e24c573d3f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1st to Die (Women's ...</td>\n",
       "      <td>53.98</td>\n",
       "      <td>One</td>\n",
       "      <td>../media/cache/2b/41/2b4161c5b72a4ae386b644682...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1,000 Places to See ...</td>\n",
       "      <td>26.08</td>\n",
       "      <td>Five</td>\n",
       "      <td>../media/cache/d7/0f/d70f7edd92705c45a82118c3f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  price star_rating  \\\n",
       "0                  A Light in the ...  51.77       Three   \n",
       "1                  Tipping the Velvet  53.74         One   \n",
       "2                          Soumission  50.10         One   \n",
       "3                       Sharp Objects  47.82        Four   \n",
       "4        Sapiens: A Brief History ...  54.23        Five   \n",
       "..                                ...    ...         ...   \n",
       "995  Alice in Wonderland (Alice's ...  55.53         One   \n",
       "996    Ajin: Demi-Human, Volume 1 ...  57.06        Four   \n",
       "997         A Spy's Devotion (The ...  16.97        Five   \n",
       "998           1st to Die (Women's ...  53.98         One   \n",
       "999           1,000 Places to See ...  26.08        Five   \n",
       "\n",
       "                                             image_url  \n",
       "0    media/cache/2c/da/2cdad67c44b002e7ead0cc35693c...  \n",
       "1    media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f...  \n",
       "2    media/cache/3e/ef/3eef99c9d9adef34639f51066202...  \n",
       "3    media/cache/32/51/3251cf3a3412f53f339e42cac213...  \n",
       "4    media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c...  \n",
       "..                                                 ...  \n",
       "995  ../media/cache/96/ee/96ee77d71a31b7694dac6855f...  \n",
       "996  ../media/cache/09/7c/097cb5ecc6fb3fbe1690cf0cb...  \n",
       "997  ../media/cache/1b/5f/1b5ff86f3c75e51e24c573d3f...  \n",
       "998  ../media/cache/2b/41/2b4161c5b72a4ae386b644682...  \n",
       "999  ../media/cache/d7/0f/d70f7edd92705c45a82118c3f...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds6001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
